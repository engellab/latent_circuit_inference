system arguments: ['/home/pt1290/latent_circuit_inference/src/run_LCI.py', '0.0082221_CDDM;relu;N=96;lmbdo=0.3;lmbdr=0.3;lr=0.002;maxiter=3000']
0.0082221_CDDM;relu;N=96;lmbdo=0.3;lmbdr=0.3;lr=0.002;maxiter=3000 0
seed: 647419
Using cuda for RNN!
Using cuda for Latent Circuit!
setting projection of RNN traces on the lower subspace
iteration 0, train loss: [92m3.763215[0m, validation loss: [92m3.250403[0m
iteration 1, train loss: [92m3.285854[0m, validation loss: [92m2.903892[0m
iteration 2, train loss: [92m2.934371[0m, validation loss: [92m2.647908[0m
iteration 3, train loss: [92m2.68544[0m, validation loss: [92m2.46132[0m
iteration 4, train loss: [92m2.50847[0m, validation loss: [92m2.328253[0m
iteration 5, train loss: [92m2.378384[0m, validation loss: [92m2.230508[0m
iteration 6, train loss: [92m2.289006[0m, validation loss: [92m2.15207[0m
iteration 7, train loss: [92m2.210055[0m, validation loss: [92m2.085336[0m
iteration 8, train loss: [92m2.141924[0m, validation loss: [92m2.028036[0m
iteration 9, train loss: [92m2.082103[0m, validation loss: [92m1.979488[0m
iteration 10, train loss: [92m2.029251[0m, validation loss: [92m1.938689[0m
iteration 11, train loss: [92m1.988692[0m, validation loss: [92m1.90478[0m
iteration 12, train loss: [92m1.947671[0m, validation loss: [92m1.876399[0m
iteration 13, train loss: [92m1.917858[0m, validation loss: [92m1.851956[0m
iteration 14, train loss: [92m1.888227[0m, validation loss: [92m1.829852[0m
iteration 15, train loss: [92m1.865064[0m, validation loss: [92m1.809009[0m
iteration 16, train loss: [92m1.846755[0m, validation loss: [92m1.788146[0m
iteration 17, train loss: [92m1.831109[0m, validation loss: [92m1.76645[0m
iteration 18, train loss: [92m1.8067[0m, validation loss: [92m1.743603[0m
iteration 19, train loss: [92m1.78769[0m, validation loss: [92m1.718912[0m
iteration 20, train loss: [92m1.770844[0m, validation loss: [92m1.692759[0m
iteration 21, train loss: [92m1.740667[0m, validation loss: [92m1.66565[0m
iteration 22, train loss: [92m1.725441[0m, validation loss: [92m1.639399[0m
iteration 23, train loss: [92m1.697811[0m, validation loss: [92m1.613237[0m
iteration 24, train loss: [92m1.67606[0m, validation loss: [92m1.588168[0m
iteration 25, train loss: [92m1.662676[0m, validation loss: [92m1.565089[0m
iteration 26, train loss: [92m1.635863[0m, validation loss: [92m1.544496[0m
iteration 27, train loss: [92m1.621847[0m, validation loss: [92m1.525362[0m
iteration 28, train loss: [92m1.598283[0m, validation loss: [92m1.506885[0m
iteration 29, train loss: [92m1.58735[0m, validation loss: [92m1.487548[0m
iteration 30, train loss: [92m1.563136[0m, validation loss: [92m1.465576[0m
iteration 31, train loss: [92m1.542243[0m, validation loss: [92m1.444767[0m
iteration 32, train loss: [92m1.527299[0m, validation loss: [92m1.425252[0m
iteration 33, train loss: [92m1.506562[0m, validation loss: [92m1.412503[0m
iteration 34, train loss: [92m1.491482[0m, validation loss: [92m1.403408[0m
iteration 35, train loss: [92m1.468119[0m, validation loss: [92m1.395959[0m
iteration 36, train loss: [92m1.46551[0m, validation loss: [92m1.383056[0m
iteration 37, train loss: [92m1.458336[0m, validation loss: [92m1.371882[0m
iteration 38, train loss: [92m1.447263[0m, validation loss: [92m1.369548[0m
iteration 39, train loss: [92m1.434769[0m, validation loss: [92m1.362363[0m
iteration 40, train loss: [92m1.426507[0m, validation loss: [92m1.347012[0m
iteration 41, train loss: [92m1.409954[0m, validation loss: [92m1.331259[0m
iteration 42, train loss: 1.416999, validation loss: [92m1.32509[0m
iteration 43, train loss: [92m1.402619[0m, validation loss: [92m1.319929[0m
iteration 44, train loss: [92m1.394456[0m, validation loss: [92m1.306[0m
iteration 45, train loss: [92m1.388007[0m, validation loss: [92m1.290731[0m
iteration 46, train loss: [92m1.380985[0m, validation loss: [92m1.284682[0m
iteration 47, train loss: [92m1.365432[0m, validation loss: [92m1.282423[0m
iteration 48, train loss: [92m1.361518[0m, validation loss: 1.283013
iteration 49, train loss: [92m1.358187[0m, validation loss: [92m1.277389[0m
iteration 50, train loss: [92m1.345708[0m, validation loss: [92m1.266151[0m
iteration 51, train loss: [92m1.338662[0m, validation loss: [92m1.256016[0m
iteration 52, train loss: [92m1.327207[0m, validation loss: [92m1.249396[0m
iteration 53, train loss: 1.33452, validation loss: [92m1.246388[0m
iteration 54, train loss: [92m1.324041[0m, validation loss: [92m1.244395[0m
iteration 55, train loss: [92m1.321842[0m, validation loss: [92m1.240496[0m
iteration 56, train loss: [92m1.315551[0m, validation loss: [92m1.232126[0m
iteration 57, train loss: 1.319243, validation loss: [92m1.223519[0m
iteration 58, train loss: [92m1.31014[0m, validation loss: [92m1.219293[0m
iteration 59, train loss: [92m1.309237[0m, validation loss: [92m1.218367[0m
iteration 60, train loss: [92m1.307498[0m, validation loss: [92m1.218151[0m
iteration 61, train loss: 1.30891, validation loss: [92m1.213645[0m
iteration 62, train loss: [92m1.2918[0m, validation loss: [92m1.206925[0m
iteration 63, train loss: 1.294039, validation loss: [92m1.202857[0m
iteration 64, train loss: [92m1.285627[0m, validation loss: [92m1.20059[0m
iteration 65, train loss: [92m1.282613[0m, validation loss: [92m1.19986[0m
iteration 66, train loss: [92m1.279215[0m, validation loss: [92m1.198411[0m
iteration 67, train loss: [92m1.276951[0m, validation loss: [92m1.192539[0m
iteration 68, train loss: 1.280003, validation loss: [92m1.184821[0m
iteration 69, train loss: [92m1.27517[0m, validation loss: [92m1.179909[0m
iteration 70, train loss: 1.280421, validation loss: [92m1.177354[0m
iteration 71, train loss: [92m1.263707[0m, validation loss: 1.178082
iteration 72, train loss: 1.265564, validation loss: 1.17815
iteration 73, train loss: 1.265579, validation loss: [92m1.173632[0m
iteration 74, train loss: [92m1.261411[0m, validation loss: [92m1.169095[0m
iteration 75, train loss: [92m1.247649[0m, validation loss: [92m1.165545[0m
iteration 76, train loss: 1.254943, validation loss: [92m1.162409[0m
iteration 77, train loss: 1.251681, validation loss: [92m1.160749[0m
iteration 78, train loss: [92m1.246806[0m, validation loss: [92m1.158593[0m
iteration 79, train loss: 1.250807, validation loss: [92m1.155528[0m
iteration 80, train loss: 1.256022, validation loss: [92m1.15307[0m
iteration 81, train loss: [92m1.24215[0m, validation loss: [92m1.151531[0m
iteration 82, train loss: [92m1.238562[0m, validation loss: [92m1.149684[0m
iteration 83, train loss: [92m1.229382[0m, validation loss: [92m1.148038[0m
iteration 84, train loss: [92m1.223313[0m, validation loss: [92m1.145341[0m
iteration 85, train loss: 1.226786, validation loss: [92m1.14199[0m
iteration 86, train loss: [92m1.216514[0m, validation loss: [92m1.139804[0m
iteration 87, train loss: 1.225384, validation loss: [92m1.138206[0m
iteration 88, train loss: 1.22195, validation loss: [92m1.136688[0m
iteration 89, train loss: 1.222871, validation loss: [92m1.135428[0m
iteration 90, train loss: [92m1.21218[0m, validation loss: [92m1.134308[0m
iteration 91, train loss: [92m1.205148[0m, validation loss: [92m1.133292[0m
iteration 92, train loss: 1.222533, validation loss: [92m1.130599[0m
iteration 93, train loss: 1.210965, validation loss: [92m1.128031[0m
iteration 94, train loss: 1.205366, validation loss: [92m1.125852[0m
iteration 95, train loss: 1.205154, validation loss: [92m1.124323[0m
iteration 96, train loss: [92m1.199793[0m, validation loss: [92m1.123439[0m
iteration 97, train loss: 1.21123, validation loss: [92m1.123185[0m
iteration 98, train loss: 1.206732, validation loss: [92m1.120011[0m
iteration 99, train loss: 1.20322, validation loss: [92m1.116411[0m
iteration 100, train loss: 1.202306, validation loss: [92m1.114687[0m
iteration 101, train loss: [92m1.193583[0m, validation loss: [92m1.113163[0m
iteration 102, train loss: 1.197981, validation loss: 1.113205
iteration 103, train loss: 1.198786, validation loss: [92m1.110788[0m
iteration 104, train loss: [92m1.186468[0m, validation loss: [92m1.108912[0m
iteration 105, train loss: [92m1.185997[0m, validation loss: [92m1.107594[0m
iteration 106, train loss: 1.188636, validation loss: [92m1.106188[0m
iteration 107, train loss: 1.202631, validation loss: [92m1.105778[0m
iteration 108, train loss: 1.186791, validation loss: [92m1.104663[0m
iteration 109, train loss: 1.196176, validation loss: [92m1.101918[0m
iteration 110, train loss: 1.201134, validation loss: [92m1.100502[0m
iteration 111, train loss: [92m1.173605[0m, validation loss: [92m1.099473[0m
iteration 112, train loss: 1.183474, validation loss: 1.100532
iteration 113, train loss: 1.178352, validation loss: [92m1.098907[0m
iteration 114, train loss: 1.18673, validation loss: [92m1.095662[0m
iteration 115, train loss: [92m1.164211[0m, validation loss: [92m1.094131[0m
iteration 116, train loss: 1.179963, validation loss: [92m1.092305[0m
iteration 117, train loss: 1.171061, validation loss: 1.092909
iteration 118, train loss: [92m1.163227[0m, validation loss: [92m1.09153[0m
iteration 119, train loss: 1.170753, validation loss: [92m1.090274[0m
iteration 120, train loss: 1.17577, validation loss: [92m1.089082[0m
iteration 121, train loss: 1.170898, validation loss: 1.089859
iteration 122, train loss: 1.167531, validation loss: [92m1.088412[0m
iteration 123, train loss: [92m1.154368[0m, validation loss: [92m1.085407[0m
iteration 124, train loss: 1.163298, validation loss: [92m1.08428[0m
iteration 125, train loss: 1.16271, validation loss: [92m1.0825[0m
iteration 126, train loss: 1.16379, validation loss: [92m1.082335[0m
iteration 127, train loss: 1.171363, validation loss: [92m1.082067[0m
iteration 128, train loss: 1.169255, validation loss: [92m1.078948[0m
iteration 129, train loss: 1.156472, validation loss: [92m1.078427[0m
iteration 130, train loss: [92m1.152537[0m, validation loss: [92m1.077035[0m
iteration 131, train loss: [92m1.143846[0m, validation loss: 1.078156
iteration 132, train loss: 1.161264, validation loss: [92m1.075865[0m
iteration 133, train loss: 1.153328, validation loss: [92m1.074123[0m
iteration 134, train loss: 1.155819, validation loss: [92m1.073307[0m
iteration 135, train loss: 1.154975, validation loss: [92m1.072658[0m
iteration 136, train loss: [92m1.141073[0m, validation loss: [92m1.071871[0m
iteration 137, train loss: 1.158407, validation loss: [92m1.069977[0m
iteration 138, train loss: [92m1.139354[0m, validation loss: [92m1.068942[0m
iteration 139, train loss: 1.149059, validation loss: 1.069129
iteration 140, train loss: 1.164926, validation loss: [92m1.067997[0m
iteration 141, train loss: [92m1.13779[0m, validation loss: [92m1.06629[0m
iteration 142, train loss: 1.146072, validation loss: [92m1.064732[0m
iteration 143, train loss: 1.145965, validation loss: [92m1.064302[0m
iteration 144, train loss: 1.147832, validation loss: [92m1.063504[0m
iteration 145, train loss: [92m1.134058[0m, validation loss: [92m1.061526[0m
iteration 146, train loss: 1.137891, validation loss: [92m1.061109[0m
iteration 147, train loss: 1.142185, validation loss: [92m1.060703[0m
iteration 148, train loss: 1.145297, validation loss: 1.06139
iteration 149, train loss: 1.13794, validation loss: [92m1.059685[0m
iteration 150, train loss: 1.134092, validation loss: [92m1.056969[0m
iteration 151, train loss: [92m1.123784[0m, validation loss: [92m1.056035[0m
iteration 152, train loss: 1.132408, validation loss: [92m1.055874[0m
iteration 153, train loss: 1.124705, validation loss: 1.056868
iteration 154, train loss: 1.129593, validation loss: [92m1.054723[0m
iteration 155, train loss: [92m1.123552[0m, validation loss: 1.054962
iteration 156, train loss: 1.128472, validation loss: [92m1.053173[0m
iteration 157, train loss: 1.129191, validation loss: 1.053819
iteration 158, train loss: 1.133364, validation loss: [92m1.051872[0m
iteration 159, train loss: 1.130291, validation loss: [92m1.049853[0m
iteration 160, train loss: 1.12521, validation loss: 1.050133
iteration 161, train loss: 1.133132, validation loss: [92m1.04825[0m
iteration 162, train loss: 1.132296, validation loss: 1.049661
iteration 163, train loss: [92m1.114442[0m, validation loss: [92m1.045912[0m
iteration 164, train loss: [92m1.111834[0m, validation loss: [92m1.044747[0m
iteration 165, train loss: 1.12746, validation loss: [92m1.043989[0m
iteration 166, train loss: 1.126409, validation loss: 1.044295
iteration 167, train loss: 1.13177, validation loss: [92m1.043753[0m
iteration 168, train loss: 1.12438, validation loss: [92m1.042406[0m
iteration 169, train loss: [92m1.104789[0m, validation loss: [92m1.042047[0m
iteration 170, train loss: 1.116492, validation loss: [92m1.041276[0m
iteration 171, train loss: 1.12685, validation loss: 1.04227
iteration 172, train loss: 1.127855, validation loss: [92m1.039214[0m
iteration 173, train loss: 1.124347, validation loss: 1.039238
iteration 174, train loss: [92m1.103357[0m, validation loss: [92m1.038678[0m
iteration 175, train loss: [92m1.101223[0m, validation loss: 1.039423
iteration 176, train loss: 1.105513, validation loss: [92m1.038029[0m
iteration 177, train loss: 1.11958, validation loss: [92m1.036268[0m
iteration 178, train loss: 1.119338, validation loss: [92m1.034978[0m
iteration 179, train loss: 1.122015, validation loss: [92m1.034138[0m
iteration 180, train loss: 1.122442, validation loss: [92m1.033971[0m
iteration 181, train loss: 1.121413, validation loss: [92m1.03357[0m
iteration 182, train loss: 1.114841, validation loss: [92m1.033263[0m
iteration 183, train loss: 1.10639, validation loss: [92m1.032348[0m
iteration 184, train loss: 1.112407, validation loss: [92m1.031361[0m
iteration 185, train loss: 1.110598, validation loss: [92m1.030859[0m
iteration 186, train loss: [92m1.092482[0m, validation loss: [92m1.029786[0m
iteration 187, train loss: 1.096207, validation loss: [92m1.029241[0m
iteration 188, train loss: 1.103409, validation loss: [92m1.028826[0m
iteration 189, train loss: 1.102628, validation loss: 1.030723
iteration 190, train loss: 1.10087, validation loss: 1.030882
iteration 191, train loss: 1.108474, validation loss: 1.029239
iteration 192, train loss: 1.104181, validation loss: [92m1.028377[0m
iteration 193, train loss: 1.101842, validation loss: [92m1.027026[0m
iteration 194, train loss: [92m1.090585[0m, validation loss: [92m1.026098[0m
iteration 195, train loss: 1.096669, validation loss: [92m1.025017[0m
iteration 196, train loss: [92m1.090327[0m, validation loss: [92m1.024538[0m
iteration 197, train loss: 1.106644, validation loss: [92m1.024533[0m
iteration 198, train loss: 1.105678, validation loss: 1.025867
iteration 199, train loss: 1.119633, validation loss: [92m1.024283[0m
iteration 200, train loss: 1.110326, validation loss: [92m1.022143[0m
iteration 201, train loss: 1.101122, validation loss: [92m1.020816[0m
iteration 202, train loss: 1.105581, validation loss: [92m1.020245[0m
iteration 203, train loss: 1.104797, validation loss: 1.02055
iteration 204, train loss: 1.113311, validation loss: 1.020783
iteration 205, train loss: [92m1.090327[0m, validation loss: 1.020952
iteration 206, train loss: 1.091554, validation loss: 1.020602
iteration 207, train loss: 1.092825, validation loss: [92m1.019793[0m
iteration 208, train loss: 1.095527, validation loss: [92m1.019664[0m
iteration 209, train loss: 1.093728, validation loss: [92m1.018157[0m
iteration 210, train loss: [92m1.077568[0m, validation loss: [92m1.017805[0m
iteration 211, train loss: 1.11346, validation loss: 1.018659
iteration 212, train loss: 1.082541, validation loss: 1.018261
iteration 213, train loss: 1.090597, validation loss: [92m1.017323[0m
iteration 214, train loss: 1.099736, validation loss: [92m1.01639[0m
iteration 215, train loss: 1.087528, validation loss: [92m1.015882[0m
iteration 216, train loss: 1.082488, validation loss: 1.016168
iteration 217, train loss: 1.086673, validation loss: [92m1.015756[0m
iteration 218, train loss: 1.102654, validation loss: [92m1.015376[0m
iteration 219, train loss: 1.09274, validation loss: [92m1.015198[0m
iteration 220, train loss: 1.117656, validation loss: 1.016052
iteration 221, train loss: 1.085064, validation loss: [92m1.014997[0m
iteration 222, train loss: 1.091102, validation loss: [92m1.013144[0m
iteration 223, train loss: 1.082183, validation loss: [92m1.012867[0m
iteration 224, train loss: 1.078031, validation loss: 1.017452
iteration 225, train loss: 1.095328, validation loss: 1.018099
iteration 226, train loss: 1.104873, validation loss: 1.013804
iteration 227, train loss: 1.079244, validation loss: 1.014381
iteration 228, train loss: 1.101067, validation loss: [92m1.011541[0m
iteration 229, train loss: 1.092434, validation loss: 1.01476
iteration 230, train loss: 1.093321, validation loss: 1.012657
iteration 231, train loss: 1.083921, validation loss: [92m1.01087[0m
iteration 232, train loss: 1.090114, validation loss: 1.011157
iteration 233, train loss: [92m1.072149[0m, validation loss: [92m1.010663[0m
iteration 234, train loss: 1.089731, validation loss: 1.016306
iteration 235, train loss: 1.099691, validation loss: [92m1.010553[0m
iteration 236, train loss: 1.095077, validation loss: 1.010684
iteration 237, train loss: 1.099227, validation loss: [92m1.009629[0m
iteration 238, train loss: 1.082909, validation loss: 1.014211
iteration 239, train loss: 1.097215, validation loss: 1.013944
iteration 240, train loss: 1.085149, validation loss: [92m1.009054[0m
iteration 241, train loss: 1.086546, validation loss: [92m1.008915[0m
iteration 242, train loss: 1.098353, validation loss: [92m1.008764[0m
iteration 243, train loss: 1.083136, validation loss: 1.011013
iteration 244, train loss: 1.089444, validation loss: 1.009662
iteration 245, train loss: 1.089086, validation loss: 1.009157
iteration 246, train loss: 1.087521, validation loss: 1.008997
iteration 247, train loss: 1.074899, validation loss: [92m1.008601[0m
iteration 248, train loss: 1.088372, validation loss: 1.008952
iteration 249, train loss: 1.081887, validation loss: [92m1.008508[0m
iteration 250, train loss: [92m1.07101[0m, validation loss: [92m1.006143[0m
iteration 251, train loss: 1.076435, validation loss: [92m1.006115[0m
iteration 252, train loss: 1.088685, validation loss: 1.00703
iteration 253, train loss: 1.085087, validation loss: 1.009002
iteration 254, train loss: 1.076453, validation loss: 1.007697
iteration 255, train loss: 1.085879, validation loss: [92m1.004696[0m
iteration 256, train loss: 1.072504, validation loss: [92m1.003646[0m
iteration 257, train loss: 1.085986, validation loss: 1.004705
iteration 258, train loss: 1.072673, validation loss: 1.007885
iteration 259, train loss: 1.089581, validation loss: 1.004203
iteration 260, train loss: 1.078621, validation loss: [92m1.002639[0m
iteration 261, train loss: 1.081968, validation loss: [92m1.002212[0m
iteration 262, train loss: 1.076424, validation loss: 1.004051
iteration 263, train loss: 1.087426, validation loss: 1.002673
iteration 264, train loss: 1.074427, validation loss: [92m0.99889[0m
iteration 265, train loss: 1.080603, validation loss: [92m0.997841[0m
iteration 266, train loss: 1.081035, validation loss: 0.998271
iteration 267, train loss: 1.077322, validation loss: 0.999441
iteration 268, train loss: [92m1.062728[0m, validation loss: 0.997966
iteration 269, train loss: 1.07812, validation loss: [92m0.996355[0m
iteration 270, train loss: 1.077359, validation loss: [92m0.994913[0m
iteration 271, train loss: 1.082749, validation loss: [92m0.994059[0m
iteration 272, train loss: 1.068527, validation loss: [92m0.992965[0m
iteration 273, train loss: 1.075263, validation loss: [92m0.990471[0m
iteration 274, train loss: 1.078958, validation loss: [92m0.98743[0m
iteration 275, train loss: [92m1.059529[0m, validation loss: [92m0.984491[0m
iteration 276, train loss: [92m1.048247[0m, validation loss: [92m0.98227[0m
iteration 277, train loss: 1.070394, validation loss: 0.983943
iteration 278, train loss: 1.068964, validation loss: [92m0.979051[0m
iteration 279, train loss: 1.071393, validation loss: [92m0.976819[0m
iteration 280, train loss: 1.071762, validation loss: [92m0.974544[0m
iteration 281, train loss: 1.063346, validation loss: [92m0.970301[0m
iteration 282, train loss: 1.06117, validation loss: 0.977696
iteration 283, train loss: 1.057259, validation loss: [92m0.964723[0m
iteration 284, train loss: 1.058824, validation loss: [92m0.961479[0m
iteration 285, train loss: 1.054023, validation loss: 0.983972
iteration 286, train loss: 1.049352, validation loss: [92m0.960427[0m
iteration 287, train loss: 1.052812, validation loss: [92m0.953317[0m
iteration 288, train loss: [92m1.04494[0m, validation loss: [92m0.950079[0m
iteration 289, train loss: 1.050804, validation loss: [92m0.946033[0m
iteration 290, train loss: [92m1.03762[0m, validation loss: 0.968
iteration 291, train loss: 1.041892, validation loss: [92m0.944229[0m
iteration 292, train loss: 1.050667, validation loss: 0.944295
iteration 293, train loss: [92m1.035859[0m, validation loss: [92m0.941817[0m
iteration 294, train loss: 1.048967, validation loss: [92m0.939901[0m
iteration 295, train loss: 1.038803, validation loss: [92m0.938771[0m
iteration 296, train loss: 1.037962, validation loss: 0.954816
iteration 297, train loss: [92m1.030024[0m, validation loss: [92m0.931168[0m
iteration 298, train loss: 1.04037, validation loss: [92m0.929949[0m
iteration 299, train loss: [92m1.023905[0m, validation loss: 0.930729
iteration 300, train loss: 1.040027, validation loss: [92m0.926058[0m
iteration 301, train loss: 1.026734, validation loss: [92m0.924282[0m
iteration 302, train loss: 1.031491, validation loss: 0.941026
iteration 303, train loss: 1.042806, validation loss: 0.988149
iteration 304, train loss: 1.055197, validation loss: 0.929484
iteration 305, train loss: [92m1.016872[0m, validation loss: 0.968146
iteration 306, train loss: 1.056839, validation loss: 0.969859
iteration 307, train loss: 1.066427, validation loss: 0.949119
iteration 308, train loss: 1.053024, validation loss: 0.983723
iteration 309, train loss: 1.063734, validation loss: 0.933691
iteration 310, train loss: 1.037747, validation loss: 0.970236
iteration 311, train loss: 1.053015, validation loss: 0.942383
iteration 312, train loss: 1.032187, validation loss: 0.95263
iteration 313, train loss: 1.033507, validation loss: 0.941752
iteration 314, train loss: 1.029066, validation loss: 0.950112
iteration 315, train loss: 1.053379, validation loss: 0.941567
iteration 316, train loss: 1.032926, validation loss: 0.939029
iteration 317, train loss: [92m1.011594[0m, validation loss: 0.924995
iteration 318, train loss: 1.018266, validation loss: 0.933646
iteration 319, train loss: 1.026353, validation loss: 0.925924
iteration 320, train loss: 1.048082, validation loss: 0.931772
iteration 321, train loss: 1.035244, validation loss: [92m0.921431[0m
iteration 322, train loss: 1.04101, validation loss: [92m0.916774[0m
iteration 323, train loss: 1.019429, validation loss: 0.922927
iteration 324, train loss: 1.016982, validation loss: [92m0.915167[0m
iteration 325, train loss: 1.025655, validation loss: [92m0.914581[0m
iteration 326, train loss: 1.023886, validation loss: [92m0.912214[0m
iteration 327, train loss: [92m1.005298[0m, validation loss: [92m0.910665[0m
iteration 328, train loss: 1.008981, validation loss: [92m0.910016[0m
iteration 329, train loss: 1.00818, validation loss: [92m0.909909[0m
iteration 330, train loss: 1.027527, validation loss: [92m0.909502[0m
iteration 331, train loss: 1.015632, validation loss: 0.912565
iteration 332, train loss: 1.013085, validation loss: 0.915016
iteration 333, train loss: 1.010418, validation loss: [92m0.90773[0m
iteration 334, train loss: 1.031655, validation loss: 0.925555
iteration 335, train loss: 1.005622, validation loss: [92m0.906492[0m
iteration 336, train loss: 1.021747, validation loss: 0.935634
iteration 337, train loss: 1.032627, validation loss: 0.906591
iteration 338, train loss: 1.016792, validation loss: 0.927537
iteration 339, train loss: 1.016533, validation loss: 0.911402
iteration 340, train loss: 1.025162, validation loss: 0.925104
iteration 341, train loss: 1.009829, validation loss: 0.914361
iteration 342, train loss: 1.031671, validation loss: 0.911368
iteration 343, train loss: 1.009668, validation loss: 0.913512
iteration 344, train loss: 1.036387, validation loss: [92m0.905399[0m
iteration 345, train loss: [92m1.003192[0m, validation loss: 0.90939
iteration 346, train loss: [92m0.996907[0m, validation loss: 0.908162
iteration 347, train loss: 1.019064, validation loss: 0.906232
iteration 348, train loss: 1.010106, validation loss: 0.911671
iteration 349, train loss: 1.032894, validation loss: 0.905768
iteration 350, train loss: [92m0.993697[0m, validation loss: [92m0.899825[0m
iteration 351, train loss: 1.011336, validation loss: [92m0.89781[0m
iteration 352, train loss: [92m0.988716[0m, validation loss: [92m0.897719[0m
iteration 353, train loss: 0.994686, validation loss: 0.908752
iteration 354, train loss: 1.010291, validation loss: [92m0.894546[0m
iteration 355, train loss: 1.023449, validation loss: 0.901621
iteration 356, train loss: 0.99767, validation loss: 0.898172
iteration 357, train loss: 0.993068, validation loss: 0.90642
iteration 358, train loss: 1.020423, validation loss: 0.89887
iteration 359, train loss: 0.996704, validation loss: 0.9012
iteration 360, train loss: 0.99295, validation loss: 0.89823
iteration 361, train loss: 0.991681, validation loss: [92m0.893177[0m
iteration 362, train loss: 1.027639, validation loss: [92m0.890971[0m
iteration 363, train loss: 0.999923, validation loss: 0.89236
iteration 364, train loss: 1.014196, validation loss: [92m0.890875[0m
iteration 365, train loss: 1.002005, validation loss: 0.892971
iteration 366, train loss: 0.990935, validation loss: 0.893693
iteration 367, train loss: 0.993399, validation loss: 0.902953
iteration 368, train loss: 1.000532, validation loss: [92m0.890707[0m
iteration 369, train loss: 0.994695, validation loss: 0.891458
iteration 370, train loss: [92m0.988488[0m, validation loss: [92m0.888681[0m
iteration 371, train loss: [92m0.986879[0m, validation loss: 0.899293
iteration 372, train loss: 1.001162, validation loss: [92m0.885466[0m
iteration 373, train loss: 1.002739, validation loss: 0.907318
iteration 374, train loss: 1.010129, validation loss: 0.885926
iteration 375, train loss: 1.01552, validation loss: 0.903917
iteration 376, train loss: 1.009342, validation loss: 0.89145
iteration 377, train loss: [92m0.978885[0m, validation loss: 0.90483
iteration 378, train loss: 1.013266, validation loss: 0.886893
iteration 379, train loss: 0.992654, validation loss: 0.889279
iteration 380, train loss: 1.013745, validation loss: [92m0.876682[0m
iteration 381, train loss: 0.993909, validation loss: 0.885117
iteration 382, train loss: 0.99882, validation loss: [92m0.873263[0m
iteration 383, train loss: 0.997144, validation loss: 0.89042
iteration 384, train loss: 0.988426, validation loss: [92m0.870482[0m
iteration 385, train loss: 1.00024, validation loss: 0.897533
iteration 386, train loss: 1.008604, validation loss: [92m0.867151[0m
iteration 387, train loss: 0.986403, validation loss: 0.874573
iteration 388, train loss: 0.994347, validation loss: [92m0.853082[0m
iteration 389, train loss: 0.986558, validation loss: 0.868123
iteration 390, train loss: 0.986526, validation loss: [92m0.841253[0m
iteration 391, train loss: [92m0.969906[0m, validation loss: 0.842809
iteration 392, train loss: 0.973963, validation loss: [92m0.825672[0m
iteration 393, train loss: [92m0.956426[0m, validation loss: 0.830446
iteration 394, train loss: [92m0.951954[0m, validation loss: [92m0.81317[0m
iteration 395, train loss: [92m0.929912[0m, validation loss: [92m0.809141[0m
iteration 396, train loss: 0.93947, validation loss: [92m0.785254[0m
iteration 397, train loss: [92m0.911764[0m, validation loss: 0.79291
iteration 398, train loss: 0.918746, validation loss: [92m0.762338[0m
iteration 399, train loss: [92m0.893261[0m, validation loss: [92m0.754729[0m
iteration 400, train loss: [92m0.893231[0m, validation loss: [92m0.732814[0m
iteration 401, train loss: [92m0.887984[0m, validation loss: 0.75239
iteration 402, train loss: [92m0.879787[0m, validation loss: 0.736179
iteration 403, train loss: 0.890299, validation loss: [92m0.712854[0m
iteration 404, train loss: [92m0.833356[0m, validation loss: 0.72319
iteration 405, train loss: 0.856203, validation loss: [92m0.665414[0m
iteration 406, train loss: [92m0.802169[0m, validation loss: 0.677636
iteration 407, train loss: [92m0.78487[0m, validation loss: [92m0.656751[0m
iteration 408, train loss: 0.831309, validation loss: 0.699482
iteration 409, train loss: 0.830326, validation loss: 0.74463
iteration 410, train loss: 0.803264, validation loss: [92m0.628955[0m
iteration 411, train loss: [92m0.775817[0m, validation loss: [92m0.617973[0m
iteration 412, train loss: 0.782305, validation loss: 0.631679
iteration 413, train loss: [92m0.755504[0m, validation loss: [92m0.575255[0m
iteration 414, train loss: [92m0.732388[0m, validation loss: [92m0.512487[0m
iteration 415, train loss: [92m0.689886[0m, validation loss: 0.557833
iteration 416, train loss: [92m0.683048[0m, validation loss: 0.517964
iteration 417, train loss: [92m0.632756[0m, validation loss: [92m0.480164[0m
iteration 418, train loss: 0.693101, validation loss: 0.501863
iteration 419, train loss: 0.637508, validation loss: 0.505682
iteration 420, train loss: [92m0.610636[0m, validation loss: [92m0.40577[0m
iteration 421, train loss: [92m0.569356[0m, validation loss: [92m0.37298[0m
iteration 422, train loss: 0.599569, validation loss: 0.471233
iteration 423, train loss: 0.594599, validation loss: [92m0.362033[0m
iteration 424, train loss: [92m0.535805[0m, validation loss: 0.382815
iteration 425, train loss: [92m0.512126[0m, validation loss: 0.394664
iteration 426, train loss: [92m0.475268[0m, validation loss: [92m0.293219[0m
iteration 427, train loss: 0.509763, validation loss: 0.329769
iteration 428, train loss: [92m0.469364[0m, validation loss: 0.450519
iteration 429, train loss: 0.480547, validation loss: 0.394315
iteration 430, train loss: 0.531226, validation loss: [92m0.260481[0m
iteration 431, train loss: [92m0.443005[0m, validation loss: 0.47382
iteration 432, train loss: 0.561386, validation loss: 0.364261
iteration 433, train loss: 0.500351, validation loss: 0.439579
iteration 434, train loss: 0.551515, validation loss: 0.290423
iteration 435, train loss: 0.463953, validation loss: 0.308996
iteration 436, train loss: 0.492936, validation loss: 0.281898
iteration 437, train loss: 0.461039, validation loss: [92m0.247907[0m
iteration 438, train loss: [92m0.432596[0m, validation loss: 0.277628
iteration 439, train loss: 0.486397, validation loss: 0.265971
iteration 440, train loss: 0.452413, validation loss: [92m0.226449[0m
iteration 441, train loss: [92m0.421377[0m, validation loss: 0.241809
iteration 442, train loss: 0.433228, validation loss: 0.241702
iteration 443, train loss: 0.438012, validation loss: 0.24183
iteration 444, train loss: 0.440807, validation loss: [92m0.212541[0m
iteration 445, train loss: 0.426906, validation loss: 0.289073
iteration 446, train loss: 0.423013, validation loss: 0.237521
iteration 447, train loss: 0.42631, validation loss: 0.218327
iteration 448, train loss: 0.421955, validation loss: [92m0.209036[0m
iteration 449, train loss: [92m0.395719[0m, validation loss: 0.248857
iteration 450, train loss: 0.416283, validation loss: 0.275175
iteration 451, train loss: 0.423848, validation loss: [92m0.204448[0m
iteration 452, train loss: 0.399167, validation loss: 0.238063
iteration 453, train loss: 0.410423, validation loss: [92m0.198218[0m
iteration 454, train loss: [92m0.392421[0m, validation loss: 0.335408
iteration 455, train loss: 0.447493, validation loss: 0.242454
iteration 456, train loss: [92m0.373783[0m, validation loss: 0.215818
iteration 457, train loss: 0.399139, validation loss: 0.245371
iteration 458, train loss: 0.416711, validation loss: 0.290617
iteration 459, train loss: 0.396555, validation loss: 0.263763
iteration 460, train loss: 0.477445, validation loss: 0.20476
iteration 461, train loss: 0.39669, validation loss: 0.227106
iteration 462, train loss: 0.386477, validation loss: 0.265396
iteration 463, train loss: 0.38937, validation loss: 0.211406
iteration 464, train loss: [92m0.364063[0m, validation loss: 0.234637
iteration 465, train loss: 0.41212, validation loss: 0.230819
iteration 466, train loss: 0.37819, validation loss: 0.37142
iteration 467, train loss: 0.412224, validation loss: 0.201366
iteration 468, train loss: 0.396326, validation loss: 0.239552
iteration 469, train loss: 0.417541, validation loss: [92m0.193518[0m
iteration 470, train loss: 0.407594, validation loss: 0.332999
iteration 471, train loss: 0.389845, validation loss: 0.305178
iteration 472, train loss: 0.371767, validation loss: [92m0.176062[0m
iteration 473, train loss: 0.381543, validation loss: 0.188336
iteration 474, train loss: 0.408172, validation loss: 0.206946
iteration 475, train loss: 0.399068, validation loss: 0.257357
iteration 476, train loss: 0.382804, validation loss: 0.215795
iteration 477, train loss: 0.376271, validation loss: 0.185118
iteration 478, train loss: 0.385408, validation loss: 0.182767
iteration 479, train loss: 0.369748, validation loss: 0.204101
iteration 480, train loss: 0.376291, validation loss: 0.187498
iteration 481, train loss: 0.365512, validation loss: 0.199743
iteration 482, train loss: 0.389907, validation loss: 0.17853
iteration 483, train loss: 0.394794, validation loss: [92m0.175791[0m
iteration 484, train loss: 0.402492, validation loss: 0.232282
iteration 485, train loss: 0.366811, validation loss: 0.283272
iteration 486, train loss: 0.389104, validation loss: 0.188679
iteration 487, train loss: 0.375618, validation loss: 0.190149
iteration 488, train loss: [92m0.361621[0m, validation loss: 0.191407
iteration 489, train loss: [92m0.34064[0m, validation loss: 0.263316
iteration 490, train loss: 0.407435, validation loss: 0.181877
iteration 491, train loss: 0.369005, validation loss: 0.181123
iteration 492, train loss: 0.382234, validation loss: 0.203546
iteration 493, train loss: 0.343226, validation loss: 0.258905
iteration 494, train loss: 0.370868, validation loss: [92m0.16858[0m
iteration 495, train loss: 0.35879, validation loss: [92m0.161925[0m
iteration 496, train loss: 0.375162, validation loss: 0.231306
iteration 497, train loss: [92m0.336081[0m, validation loss: 0.296258
iteration 498, train loss: 0.390307, validation loss: 0.169306
iteration 499, train loss: 0.354484, validation loss: 0.176354
iteration 500, train loss: 0.384952, validation loss: 0.166484
iteration 501, train loss: 0.38711, validation loss: 0.27868
iteration 502, train loss: 0.359809, validation loss: 0.292524
iteration 503, train loss: 0.406321, validation loss: 0.192178
iteration 504, train loss: 0.364249, validation loss: 0.174336
iteration 505, train loss: 0.362506, validation loss: 0.209835
iteration 506, train loss: 0.366446, validation loss: 0.328943
iteration 507, train loss: 0.434964, validation loss: 0.182066
iteration 508, train loss: 0.348302, validation loss: 0.205156
iteration 509, train loss: 0.406135, validation loss: 0.163996
iteration 510, train loss: 0.351552, validation loss: 0.242272
iteration 511, train loss: 0.376024, validation loss: 0.315204
iteration 512, train loss: 0.386279, validation loss: 0.243134
iteration 513, train loss: 0.369015, validation loss: 0.168966
iteration 514, train loss: 0.359286, validation loss: 0.191756
iteration 515, train loss: 0.38432, validation loss: 0.185667
iteration 516, train loss: 0.358642, validation loss: 0.267225
iteration 517, train loss: 0.379693, validation loss: 0.191714
iteration 518, train loss: 0.36966, validation loss: 0.166394
iteration 519, train loss: 0.373077, validation loss: [92m0.160483[0m
iteration 520, train loss: 0.345429, validation loss: 0.195451
iteration 521, train loss: 0.354803, validation loss: 0.237181
iteration 522, train loss: 0.36595, validation loss: 0.189206
iteration 523, train loss: 0.377493, validation loss: 0.160843
iteration 524, train loss: 0.345702, validation loss: [92m0.159178[0m
iteration 525, train loss: [92m0.333955[0m, validation loss: 0.183528
iteration 526, train loss: 0.334831, validation loss: 0.179146
iteration 527, train loss: 0.338014, validation loss: 0.184737
iteration 528, train loss: 0.363394, validation loss: 0.206029
iteration 529, train loss: 0.355194, validation loss: 0.18484
iteration 530, train loss: 0.361714, validation loss: 0.16993
iteration 531, train loss: 0.367415, validation loss: 0.182886
iteration 532, train loss: 0.36403, validation loss: 0.237273
iteration 533, train loss: 0.358937, validation loss: 0.247225
iteration 534, train loss: 0.367648, validation loss: 0.173345
iteration 535, train loss: 0.35255, validation loss: 0.168248
iteration 536, train loss: 0.360288, validation loss: 0.181384
iteration 537, train loss: [92m0.325818[0m, validation loss: 0.284253
iteration 538, train loss: 0.340794, validation loss: 0.190705
iteration 539, train loss: 0.350637, validation loss: 0.162285
iteration 540, train loss: 0.328189, validation loss: 0.178148
iteration 541, train loss: 0.365891, validation loss: 0.200919
iteration 542, train loss: 0.361263, validation loss: 0.289248
iteration 543, train loss: 0.358007, validation loss: 0.183514
iteration 544, train loss: 0.350507, validation loss: 0.167504
iteration 545, train loss: 0.359617, validation loss: 0.180011
iteration 546, train loss: 0.380053, validation loss: 0.198591
iteration 547, train loss: 0.349287, validation loss: 0.213571
iteration 548, train loss: 0.366536, validation loss: 0.16847
iteration 549, train loss: 0.346548, validation loss: [92m0.145684[0m
iteration 550, train loss: 0.362393, validation loss: 0.177345
iteration 551, train loss: 0.349381, validation loss: 0.208996
iteration 552, train loss: 0.354611, validation loss: 0.200327
iteration 553, train loss: 0.360578, validation loss: 0.169259
iteration 554, train loss: 0.36672, validation loss: 0.15911
iteration 555, train loss: 0.366546, validation loss: 0.188938
iteration 556, train loss: 0.346128, validation loss: 0.301085
iteration 557, train loss: 0.358089, validation loss: 0.22065
iteration 558, train loss: 0.38707, validation loss: 0.163796
iteration 559, train loss: 0.363442, validation loss: 0.151881
iteration 560, train loss: 0.372572, validation loss: 0.156895
iteration 561, train loss: 0.363338, validation loss: 0.186276
iteration 562, train loss: 0.347153, validation loss: 0.184141
iteration 563, train loss: 0.356279, validation loss: 0.170278
iteration 564, train loss: 0.360057, validation loss: 0.152921
iteration 565, train loss: 0.343679, validation loss: 0.19292
iteration 566, train loss: 0.348449, validation loss: 0.180201
iteration 567, train loss: 0.352313, validation loss: 0.154677
iteration 568, train loss: 0.352373, validation loss: 0.155842
iteration 569, train loss: 0.351795, validation loss: 0.164372
iteration 570, train loss: 0.34233, validation loss: 0.188598
iteration 571, train loss: 0.3343, validation loss: 0.186344
iteration 572, train loss: 0.344119, validation loss: 0.162444
iteration 573, train loss: 0.334444, validation loss: [92m0.144349[0m
iteration 574, train loss: 0.350612, validation loss: 0.148951
iteration 575, train loss: 0.330253, validation loss: 0.171929
iteration 576, train loss: 0.328226, validation loss: 0.211827
iteration 577, train loss: 0.363111, validation loss: 0.20164
iteration 578, train loss: 0.373843, validation loss: 0.145445
iteration 579, train loss: 0.341163, validation loss: 0.187476
iteration 580, train loss: 0.367031, validation loss: 0.153535
iteration 581, train loss: 0.353124, validation loss: 0.178795
iteration 582, train loss: [92m0.32181[0m, validation loss: 0.23856
iteration 583, train loss: 0.394471, validation loss: 0.162055
iteration 584, train loss: 0.342624, validation loss: 0.19279
iteration 585, train loss: 0.395956, validation loss: 0.19703
iteration 586, train loss: 0.393907, validation loss: 0.162864
iteration 587, train loss: 0.348832, validation loss: 0.283972
iteration 588, train loss: 0.391982, validation loss: 0.280355
iteration 589, train loss: 0.407595, validation loss: 0.158383
iteration 590, train loss: 0.370828, validation loss: 0.180479
iteration 591, train loss: 0.346417, validation loss: 0.1977
iteration 592, train loss: 0.394062, validation loss: 0.197241
iteration 593, train loss: 0.33677, validation loss: 0.243948
iteration 594, train loss: 0.356319, validation loss: 0.230295
iteration 595, train loss: 0.364789, validation loss: 0.187047
iteration 596, train loss: 0.344352, validation loss: 0.172867
iteration 597, train loss: 0.35426, validation loss: 0.16988
iteration 598, train loss: 0.351142, validation loss: 0.233466
iteration 599, train loss: 0.381594, validation loss: 0.168628
MSE: 0.41802715
Total R2: -4.585620950828419
Projected R2: -5.131182649088958
Plotting random trials
Analyzing fixed points
Calculating Line Attractor analytics
Analyzing points on a line attractor in motion context...
Analyzing points on a line attractor in color context...
0.0082221_CDDM;relu;N=96;lmbdo=0.3;lmbdr=0.3;lr=0.002;maxiter=3000 1
seed: 741959
Using cuda for RNN!
Using cuda for Latent Circuit!
setting projection of RNN traces on the lower subspace
iteration 0, train loss: [92m3.271194[0m, validation loss: [92m2.850906[0m
iteration 1, train loss: [92m2.919874[0m, validation loss: [92m2.592903[0m
iteration 2, train loss: [92m2.660668[0m, validation loss: [92m2.402341[0m
iteration 3, train loss: [92m2.471333[0m, validation loss: [92m2.264709[0m
iteration 4, train loss: [92m2.343184[0m, validation loss: [92m2.163783[0m
iteration 5, train loss: [92m2.245706[0m, validation loss: [92m2.083491[0m
iteration 6, train loss: [92m2.167663[0m, validation loss: [92m2.014649[0m
iteration 7, train loss: [92m2.100981[0m, validation loss: [92m1.952398[0m
iteration 8, train loss: [92m2.037067[0m, validation loss: [92m1.895532[0m
iteration 9, train loss: [92m1.981874[0m, validation loss: [92m1.845319[0m
iteration 10, train loss: [92m1.931922[0m, validation loss: [92m1.802503[0m
iteration 11, train loss: [92m1.89658[0m, validation loss: [92m1.766196[0m
iteration 12, train loss: [92m1.851524[0m, validation loss: [92m1.735898[0m
iteration 13, train loss: [92m1.81948[0m, validation loss: [92m1.709306[0m
iteration 14, train loss: [92m1.79645[0m, validation loss: [92m1.685058[0m
iteration 15, train loss: [92m1.777642[0m, validation loss: [92m1.663991[0m
iteration 16, train loss: [92m1.750832[0m, validation loss: [92m1.646145[0m
iteration 17, train loss: [92m1.728629[0m, validation loss: [92m1.628516[0m
iteration 18, train loss: [92m1.714852[0m, validation loss: [92m1.605779[0m
iteration 19, train loss: [92m1.687424[0m, validation loss: [92m1.580646[0m
iteration 20, train loss: [92m1.665089[0m, validation loss: [92m1.557614[0m
iteration 21, train loss: [92m1.651568[0m, validation loss: [92m1.537974[0m
iteration 22, train loss: [92m1.620701[0m, validation loss: [92m1.518019[0m
iteration 23, train loss: [92m1.608097[0m, validation loss: [92m1.494995[0m
iteration 24, train loss: [92m1.585727[0m, validation loss: [92m1.471653[0m
iteration 25, train loss: [92m1.565244[0m, validation loss: [92m1.448997[0m
iteration 26, train loss: [92m1.545008[0m, validation loss: [92m1.428999[0m
iteration 27, train loss: [92m1.523769[0m, validation loss: [92m1.409316[0m
iteration 28, train loss: [92m1.514724[0m, validation loss: [92m1.389168[0m
iteration 29, train loss: [92m1.497324[0m, validation loss: [92m1.368556[0m
iteration 30, train loss: [92m1.483874[0m, validation loss: [92m1.350442[0m
iteration 31, train loss: [92m1.447456[0m, validation loss: [92m1.333469[0m
iteration 32, train loss: [92m1.427644[0m, validation loss: [92m1.31355[0m
iteration 33, train loss: [92m1.414927[0m, validation loss: [92m1.300227[0m
iteration 34, train loss: [92m1.396296[0m, validation loss: [92m1.281316[0m
iteration 35, train loss: [92m1.389648[0m, validation loss: [92m1.261533[0m
iteration 36, train loss: [92m1.369432[0m, validation loss: [92m1.255098[0m
iteration 37, train loss: [92m1.368341[0m, validation loss: [92m1.252629[0m
iteration 38, train loss: [92m1.358169[0m, validation loss: [92m1.233172[0m
iteration 39, train loss: [92m1.346603[0m, validation loss: [92m1.220202[0m
iteration 40, train loss: [92m1.334252[0m, validation loss: [92m1.215669[0m
iteration 41, train loss: [92m1.332289[0m, validation loss: [92m1.211227[0m
iteration 42, train loss: [92m1.324322[0m, validation loss: [92m1.201626[0m
iteration 43, train loss: [92m1.312188[0m, validation loss: [92m1.194036[0m
iteration 44, train loss: [92m1.306427[0m, validation loss: [92m1.189207[0m
iteration 45, train loss: [92m1.295003[0m, validation loss: [92m1.182376[0m
iteration 46, train loss: [92m1.292494[0m, validation loss: [92m1.17202[0m
iteration 47, train loss: [92m1.291726[0m, validation loss: [92m1.162997[0m
iteration 48, train loss: [92m1.275627[0m, validation loss: [92m1.156361[0m
iteration 49, train loss: [92m1.274249[0m, validation loss: [92m1.151662[0m
iteration 50, train loss: 1.280955, validation loss: [92m1.145053[0m
iteration 51, train loss: [92m1.257672[0m, validation loss: [92m1.136265[0m
iteration 52, train loss: [92m1.255684[0m, validation loss: [92m1.127528[0m
iteration 53, train loss: [92m1.241481[0m, validation loss: [92m1.120228[0m
iteration 54, train loss: 1.244101, validation loss: [92m1.114204[0m
iteration 55, train loss: 1.243909, validation loss: [92m1.10698[0m
iteration 56, train loss: [92m1.234851[0m, validation loss: [92m1.100631[0m
iteration 57, train loss: [92m1.219695[0m, validation loss: [92m1.093019[0m
iteration 58, train loss: 1.228734, validation loss: [92m1.085997[0m
iteration 59, train loss: [92m1.211651[0m, validation loss: [92m1.077659[0m
iteration 60, train loss: [92m1.205079[0m, validation loss: [92m1.07252[0m
iteration 61, train loss: [92m1.192589[0m, validation loss: [92m1.05911[0m
iteration 62, train loss: 1.201803, validation loss: 1.065508
iteration 63, train loss: 1.198837, validation loss: [92m1.048011[0m
iteration 64, train loss: 1.20011, validation loss: 1.068279
iteration 65, train loss: [92m1.190217[0m, validation loss: 1.052568
iteration 66, train loss: [92m1.189186[0m, validation loss: 1.053101
iteration 67, train loss: [92m1.178544[0m, validation loss: [92m1.036531[0m
iteration 68, train loss: [92m1.173522[0m, validation loss: 1.059371
iteration 69, train loss: 1.184101, validation loss: [92m1.017448[0m
iteration 70, train loss: [92m1.154072[0m, validation loss: [92m1.01554[0m
iteration 71, train loss: 1.169287, validation loss: 1.015814
iteration 72, train loss: [92m1.146485[0m, validation loss: 1.040876
iteration 73, train loss: 1.165944, validation loss: 1.019497
iteration 74, train loss: [92m1.145742[0m, validation loss: [92m1.004627[0m
iteration 75, train loss: 1.166239, validation loss: [92m0.994807[0m
iteration 76, train loss: [92m1.132029[0m, validation loss: 1.009587
iteration 77, train loss: 1.145466, validation loss: 1.002615
iteration 78, train loss: [92m1.131146[0m, validation loss: [92m0.986784[0m
iteration 79, train loss: 1.146708, validation loss: 0.990489
iteration 80, train loss: [92m1.117352[0m, validation loss: [92m0.977407[0m
iteration 81, train loss: [92m1.109028[0m, validation loss: 0.99633
iteration 82, train loss: 1.127623, validation loss: 0.980806
iteration 83, train loss: [92m1.107348[0m, validation loss: [92m0.964363[0m
iteration 84, train loss: [92m1.099209[0m, validation loss: [92m0.961689[0m
iteration 85, train loss: 1.104737, validation loss: [92m0.959177[0m
iteration 86, train loss: [92m1.097301[0m, validation loss: 0.959817
iteration 87, train loss: [92m1.094127[0m, validation loss: [92m0.942405[0m
iteration 88, train loss: [92m1.076058[0m, validation loss: [92m0.937351[0m
iteration 89, train loss: 1.084685, validation loss: 0.941804
iteration 90, train loss: 1.080665, validation loss: 0.951513
iteration 91, train loss: 1.08428, validation loss: [92m0.925537[0m
iteration 92, train loss: [92m1.059648[0m, validation loss: 0.92975
iteration 93, train loss: 1.066827, validation loss: [92m0.921723[0m
iteration 94, train loss: [92m1.050221[0m, validation loss: 0.936475
iteration 95, train loss: 1.067668, validation loss: [92m0.909091[0m
iteration 96, train loss: 1.051194, validation loss: [92m0.906903[0m
iteration 97, train loss: [92m1.030066[0m, validation loss: [92m0.900959[0m
iteration 98, train loss: [92m1.023966[0m, validation loss: [92m0.895166[0m
iteration 99, train loss: 1.025848, validation loss: [92m0.878238[0m
iteration 100, train loss: 1.028588, validation loss: [92m0.874192[0m
iteration 101, train loss: 1.029255, validation loss: 0.937477
iteration 102, train loss: 1.034585, validation loss: 0.900881
iteration 103, train loss: [92m1.022006[0m, validation loss: 0.899896
iteration 104, train loss: 1.02747, validation loss: [92m0.862413[0m
iteration 105, train loss: [92m1.011858[0m, validation loss: 0.913926
iteration 106, train loss: 1.018296, validation loss: 0.891719
iteration 107, train loss: [92m1.002595[0m, validation loss: [92m0.861299[0m
iteration 108, train loss: [92m0.99087[0m, validation loss: 0.867979
iteration 109, train loss: 1.020381, validation loss: 0.866911
iteration 110, train loss: [92m0.960317[0m, validation loss: 0.875625
iteration 111, train loss: 0.982727, validation loss: [92m0.829123[0m
iteration 112, train loss: [92m0.931733[0m, validation loss: [92m0.824805[0m
iteration 113, train loss: 0.985523, validation loss: [92m0.787795[0m
iteration 114, train loss: [92m0.917286[0m, validation loss: 0.80165
iteration 115, train loss: 0.924956, validation loss: [92m0.780096[0m
iteration 116, train loss: 0.921282, validation loss: [92m0.744712[0m
iteration 117, train loss: [92m0.88376[0m, validation loss: [92m0.735728[0m
iteration 118, train loss: 0.889279, validation loss: [92m0.720168[0m
iteration 119, train loss: 0.89471, validation loss: [92m0.677642[0m
iteration 120, train loss: [92m0.863678[0m, validation loss: [92m0.673324[0m
iteration 121, train loss: [92m0.852544[0m, validation loss: [92m0.648126[0m
iteration 122, train loss: [92m0.822818[0m, validation loss: 0.658015
iteration 123, train loss: [92m0.822248[0m, validation loss: [92m0.616827[0m
iteration 124, train loss: [92m0.795714[0m, validation loss: [92m0.599537[0m
iteration 125, train loss: [92m0.791419[0m, validation loss: 0.603917
iteration 126, train loss: [92m0.768862[0m, validation loss: [92m0.585103[0m
iteration 127, train loss: 0.789686, validation loss: [92m0.569212[0m
iteration 128, train loss: 0.778975, validation loss: [92m0.542212[0m
iteration 129, train loss: [92m0.763229[0m, validation loss: 0.557953
iteration 130, train loss: [92m0.751812[0m, validation loss: [92m0.520181[0m
iteration 131, train loss: [92m0.704163[0m, validation loss: 0.536222
iteration 132, train loss: [92m0.70016[0m, validation loss: [92m0.514753[0m
iteration 133, train loss: [92m0.693664[0m, validation loss: 0.576041
iteration 134, train loss: 0.739116, validation loss: 0.553408
iteration 135, train loss: 0.74052, validation loss: 0.600319
iteration 136, train loss: 0.727685, validation loss: 0.534397
iteration 137, train loss: [92m0.661927[0m, validation loss: 0.553176
iteration 138, train loss: 0.715507, validation loss: 0.522157
iteration 139, train loss: 0.664173, validation loss: [92m0.504863[0m
iteration 140, train loss: 0.665776, validation loss: [92m0.502355[0m
iteration 141, train loss: 0.670134, validation loss: [92m0.494482[0m
iteration 142, train loss: 0.673429, validation loss: [92m0.491781[0m
iteration 143, train loss: [92m0.626953[0m, validation loss: 0.492657
iteration 144, train loss: 0.639879, validation loss: [92m0.463115[0m
iteration 145, train loss: 0.646226, validation loss: [92m0.460892[0m
iteration 146, train loss: 0.633715, validation loss: [92m0.451671[0m
iteration 147, train loss: 0.627155, validation loss: 0.459678
iteration 148, train loss: 0.65145, validation loss: [92m0.431694[0m
iteration 149, train loss: 0.631469, validation loss: 0.433856
iteration 150, train loss: 0.633965, validation loss: [92m0.425535[0m
iteration 151, train loss: [92m0.62478[0m, validation loss: 0.451044
iteration 152, train loss: 0.628537, validation loss: [92m0.419879[0m
iteration 153, train loss: [92m0.582413[0m, validation loss: 0.421429
iteration 154, train loss: [92m0.561056[0m, validation loss: 0.425021
iteration 155, train loss: 0.562038, validation loss: [92m0.419878[0m
iteration 156, train loss: 0.610007, validation loss: [92m0.413309[0m
iteration 157, train loss: 0.585553, validation loss: 0.419166
iteration 158, train loss: 0.576054, validation loss: 0.426364
iteration 159, train loss: 0.618705, validation loss: 0.432017
iteration 160, train loss: 0.590215, validation loss: 0.44633
iteration 161, train loss: 0.596991, validation loss: 0.451798
iteration 162, train loss: 0.595953, validation loss: [92m0.407055[0m
iteration 163, train loss: 0.607969, validation loss: 0.415093
iteration 164, train loss: 0.580452, validation loss: 0.408275
iteration 165, train loss: 0.572639, validation loss: [92m0.39054[0m
iteration 166, train loss: 0.564323, validation loss: 0.401501
iteration 167, train loss: 0.592974, validation loss: [92m0.371272[0m
iteration 168, train loss: [92m0.554346[0m, validation loss: 0.388242
iteration 169, train loss: [92m0.553126[0m, validation loss: [92m0.367296[0m
iteration 170, train loss: 0.557017, validation loss: 0.39002
iteration 171, train loss: 0.557541, validation loss: 0.388906
iteration 172, train loss: 0.571327, validation loss: 0.382489
iteration 173, train loss: [92m0.534778[0m, validation loss: 0.378251
iteration 174, train loss: 0.536491, validation loss: 0.407341
iteration 175, train loss: 0.556203, validation loss: 0.372854
iteration 176, train loss: [92m0.533532[0m, validation loss: 0.387066
iteration 177, train loss: 0.552623, validation loss: 0.400141
iteration 178, train loss: 0.561914, validation loss: [92m0.349262[0m
iteration 179, train loss: [92m0.52897[0m, validation loss: 0.446539
iteration 180, train loss: 0.597511, validation loss: 0.409945
iteration 181, train loss: 0.58572, validation loss: 0.391362
iteration 182, train loss: 0.560305, validation loss: 0.419767
iteration 183, train loss: 0.5909, validation loss: [92m0.345864[0m
iteration 184, train loss: 0.534348, validation loss: 0.420214
iteration 185, train loss: 0.549012, validation loss: 0.381354
iteration 186, train loss: 0.555239, validation loss: 0.348538
iteration 187, train loss: [92m0.502501[0m, validation loss: 0.374224
iteration 188, train loss: 0.533464, validation loss: 0.356665
iteration 189, train loss: 0.537964, validation loss: [92m0.342147[0m
iteration 190, train loss: 0.503915, validation loss: 0.371915
iteration 191, train loss: 0.55767, validation loss: [92m0.341873[0m
iteration 192, train loss: 0.517559, validation loss: [92m0.328649[0m
iteration 193, train loss: 0.55108, validation loss: 0.338629
iteration 194, train loss: 0.512698, validation loss: [92m0.320412[0m
iteration 195, train loss: [92m0.497517[0m, validation loss: 0.34561
iteration 196, train loss: 0.50124, validation loss: 0.329326
iteration 197, train loss: 0.502649, validation loss: [92m0.310404[0m
iteration 198, train loss: [92m0.494639[0m, validation loss: 0.332871
iteration 199, train loss: 0.496998, validation loss: 0.327527
iteration 200, train loss: 0.522385, validation loss: 0.334199
iteration 201, train loss: 0.513722, validation loss: 0.361778
iteration 202, train loss: 0.519151, validation loss: 0.31512
iteration 203, train loss: [92m0.491083[0m, validation loss: 0.32148
iteration 204, train loss: 0.523693, validation loss: [92m0.306269[0m
iteration 205, train loss: 0.494649, validation loss: [92m0.305735[0m
iteration 206, train loss: 0.497867, validation loss: [92m0.304542[0m
iteration 207, train loss: [92m0.487603[0m, validation loss: [92m0.292812[0m
iteration 208, train loss: 0.496382, validation loss: 0.306672
iteration 209, train loss: [92m0.482468[0m, validation loss: 0.30067
iteration 210, train loss: 0.483408, validation loss: 0.308153
iteration 211, train loss: 0.524612, validation loss: [92m0.290601[0m
iteration 212, train loss: [92m0.480563[0m, validation loss: 0.327944
iteration 213, train loss: 0.502185, validation loss: [92m0.28649[0m
iteration 214, train loss: [92m0.456494[0m, validation loss: 0.364794
iteration 215, train loss: 0.506291, validation loss: 0.291195
iteration 216, train loss: 0.466156, validation loss: 0.370331
iteration 217, train loss: 0.56135, validation loss: 0.33326
iteration 218, train loss: 0.508746, validation loss: 0.310695
iteration 219, train loss: 0.494317, validation loss: 0.372187
iteration 220, train loss: 0.517517, validation loss: 0.336935
iteration 221, train loss: 0.470226, validation loss: 0.335521
iteration 222, train loss: 0.50711, validation loss: 0.331078
iteration 223, train loss: 0.513939, validation loss: [92m0.281749[0m
iteration 224, train loss: 0.456908, validation loss: 0.374557
iteration 225, train loss: 0.509932, validation loss: 0.322482
iteration 226, train loss: 0.485449, validation loss: 0.297377
iteration 227, train loss: 0.472105, validation loss: 0.346064
iteration 228, train loss: 0.526502, validation loss: [92m0.278486[0m
iteration 229, train loss: 0.476828, validation loss: 0.330869
iteration 230, train loss: 0.498065, validation loss: 0.348611
iteration 231, train loss: 0.489253, validation loss: 0.29236
iteration 232, train loss: [92m0.435574[0m, validation loss: 0.297255
iteration 233, train loss: 0.475058, validation loss: 0.323369
iteration 234, train loss: 0.512338, validation loss: [92m0.273761[0m
iteration 235, train loss: 0.449686, validation loss: 0.328012
iteration 236, train loss: 0.511593, validation loss: 0.329169
iteration 237, train loss: 0.490703, validation loss: [92m0.270144[0m
iteration 238, train loss: 0.492543, validation loss: 0.298272
iteration 239, train loss: 0.504638, validation loss: [92m0.26308[0m
iteration 240, train loss: 0.480265, validation loss: 0.287275
iteration 241, train loss: 0.45043, validation loss: 0.298455
iteration 242, train loss: 0.455554, validation loss: 0.264658
iteration 243, train loss: 0.4393, validation loss: [92m0.260244[0m
iteration 244, train loss: 0.458196, validation loss: 0.267479
iteration 245, train loss: 0.447653, validation loss: 0.260832
iteration 246, train loss: [92m0.431108[0m, validation loss: 0.284606
iteration 247, train loss: 0.482524, validation loss: [92m0.259782[0m
iteration 248, train loss: 0.452105, validation loss: [92m0.245516[0m
iteration 249, train loss: 0.442805, validation loss: 0.253153
iteration 250, train loss: 0.432967, validation loss: [92m0.242471[0m
iteration 251, train loss: 0.439896, validation loss: 0.249966
iteration 252, train loss: 0.436859, validation loss: 0.263748
iteration 253, train loss: [92m0.424022[0m, validation loss: [92m0.240586[0m
iteration 254, train loss: 0.442041, validation loss: 0.256352
iteration 255, train loss: 0.447875, validation loss: 0.244667
iteration 256, train loss: 0.441972, validation loss: 0.256699
iteration 257, train loss: 0.424689, validation loss: 0.246799
iteration 258, train loss: [92m0.390597[0m, validation loss: 0.268169
iteration 259, train loss: 0.451753, validation loss: [92m0.234037[0m
iteration 260, train loss: 0.411314, validation loss: 0.250242
iteration 261, train loss: 0.442698, validation loss: 0.244652
iteration 262, train loss: 0.425955, validation loss: 0.303434
iteration 263, train loss: 0.455629, validation loss: 0.249857
iteration 264, train loss: 0.459586, validation loss: 0.253505
iteration 265, train loss: 0.431348, validation loss: 0.239372
iteration 266, train loss: 0.406978, validation loss: 0.269085
iteration 267, train loss: 0.434147, validation loss: 0.295339
iteration 268, train loss: 0.461795, validation loss: 0.234899
iteration 269, train loss: 0.405554, validation loss: 0.243521
iteration 270, train loss: 0.417525, validation loss: 0.253402
iteration 271, train loss: 0.422831, validation loss: 0.258955
iteration 272, train loss: 0.424019, validation loss: 0.251005
iteration 273, train loss: 0.422652, validation loss: 0.237767
iteration 274, train loss: 0.402254, validation loss: [92m0.220494[0m
iteration 275, train loss: 0.434638, validation loss: 0.260464
iteration 276, train loss: 0.435308, validation loss: 0.24872
iteration 277, train loss: 0.416809, validation loss: 0.223951
iteration 278, train loss: 0.421346, validation loss: 0.230679
iteration 279, train loss: 0.408215, validation loss: 0.225273
iteration 280, train loss: 0.394032, validation loss: 0.25181
iteration 281, train loss: 0.419315, validation loss: 0.244054
iteration 282, train loss: 0.411764, validation loss: 0.224328
iteration 283, train loss: 0.415632, validation loss: 0.224857
iteration 284, train loss: 0.451633, validation loss: 0.229889
iteration 285, train loss: 0.395164, validation loss: 0.261992
iteration 286, train loss: 0.448156, validation loss: 0.233906
iteration 287, train loss: 0.421309, validation loss: 0.221315
iteration 288, train loss: [92m0.387589[0m, validation loss: 0.231221
iteration 289, train loss: 0.40105, validation loss: 0.225749
iteration 290, train loss: [92m0.385758[0m, validation loss: 0.271655
iteration 291, train loss: 0.449444, validation loss: 0.222645
iteration 292, train loss: 0.429089, validation loss: 0.225247
iteration 293, train loss: 0.401527, validation loss: 0.237425
iteration 294, train loss: 0.418121, validation loss: [92m0.214654[0m
iteration 295, train loss: 0.399586, validation loss: 0.241755
iteration 296, train loss: 0.400765, validation loss: 0.243662
iteration 297, train loss: 0.437361, validation loss: 0.21617
iteration 298, train loss: 0.404325, validation loss: 0.21815
iteration 299, train loss: 0.416269, validation loss: [92m0.213296[0m
iteration 300, train loss: [92m0.362489[0m, validation loss: 0.246426
iteration 301, train loss: 0.447597, validation loss: [92m0.206763[0m
iteration 302, train loss: 0.378265, validation loss: 0.218715
iteration 303, train loss: 0.408651, validation loss: 0.216947
iteration 304, train loss: 0.409885, validation loss: 0.21267
iteration 305, train loss: 0.389118, validation loss: 0.261989
iteration 306, train loss: 0.417027, validation loss: 0.237054
iteration 307, train loss: 0.413533, validation loss: 0.20925
iteration 308, train loss: 0.395406, validation loss: 0.244931
iteration 309, train loss: 0.449763, validation loss: 0.241176
iteration 310, train loss: 0.436563, validation loss: 0.286506
iteration 311, train loss: 0.452559, validation loss: 0.250509
iteration 312, train loss: 0.397413, validation loss: 0.243662
iteration 313, train loss: 0.437354, validation loss: 0.216059
iteration 314, train loss: 0.397745, validation loss: 0.215681
iteration 315, train loss: 0.42172, validation loss: 0.232457
iteration 316, train loss: 0.396778, validation loss: 0.209491
iteration 317, train loss: 0.406483, validation loss: 0.209345
iteration 318, train loss: 0.391285, validation loss: 0.226176
iteration 319, train loss: 0.38367, validation loss: 0.235927
iteration 320, train loss: 0.439817, validation loss: [92m0.202026[0m
iteration 321, train loss: 0.38481, validation loss: 0.230051
iteration 322, train loss: 0.403943, validation loss: [92m0.199434[0m
iteration 323, train loss: 0.388957, validation loss: 0.25227
iteration 324, train loss: 0.44117, validation loss: 0.280718
iteration 325, train loss: 0.444325, validation loss: 0.20045
iteration 326, train loss: 0.393983, validation loss: 0.20877
iteration 327, train loss: 0.414043, validation loss: 0.209562
iteration 328, train loss: 0.411139, validation loss: 0.218894
iteration 329, train loss: 0.386479, validation loss: 0.219119
iteration 330, train loss: 0.410587, validation loss: 0.220386
iteration 331, train loss: 0.409089, validation loss: [92m0.199218[0m
iteration 332, train loss: 0.400653, validation loss: 0.235482
iteration 333, train loss: 0.417526, validation loss: 0.235566
iteration 334, train loss: 0.411236, validation loss: 0.232764
iteration 335, train loss: 0.411914, validation loss: 0.278307
iteration 336, train loss: 0.460192, validation loss: 0.212199
iteration 337, train loss: 0.385139, validation loss: 0.284911
iteration 338, train loss: 0.46853, validation loss: 0.232948
iteration 339, train loss: 0.419214, validation loss: 0.228661
iteration 340, train loss: 0.418335, validation loss: 0.273493
iteration 341, train loss: 0.442212, validation loss: 0.246226
iteration 342, train loss: 0.397169, validation loss: 0.230183
iteration 343, train loss: 0.394854, validation loss: 0.261862
iteration 344, train loss: 0.435393, validation loss: 0.219542
iteration 345, train loss: 0.388838, validation loss: 0.20844
iteration 346, train loss: 0.381837, validation loss: 0.231709
iteration 347, train loss: 0.398589, validation loss: 0.227485
iteration 348, train loss: 0.430159, validation loss: 0.218527
iteration 349, train loss: 0.397777, validation loss: 0.210031
iteration 350, train loss: 0.403801, validation loss: [92m0.190753[0m
iteration 351, train loss: 0.383581, validation loss: 0.212125
iteration 352, train loss: 0.376749, validation loss: 0.213257
iteration 353, train loss: 0.390003, validation loss: 0.191012
iteration 354, train loss: 0.400246, validation loss: 0.198424
iteration 355, train loss: 0.374918, validation loss: 0.211237
iteration 356, train loss: 0.375542, validation loss: 0.202705
iteration 357, train loss: 0.376041, validation loss: 0.203321
iteration 358, train loss: 0.379103, validation loss: [92m0.19073[0m
iteration 359, train loss: 0.37327, validation loss: 0.225651
iteration 360, train loss: 0.385683, validation loss: 0.227091
iteration 361, train loss: 0.380162, validation loss: 0.200215
iteration 362, train loss: [92m0.352613[0m, validation loss: 0.196631
iteration 363, train loss: 0.417431, validation loss: 0.194147
iteration 364, train loss: 0.420163, validation loss: 0.220873
iteration 365, train loss: 0.399509, validation loss: 0.203604
iteration 366, train loss: 0.38716, validation loss: 0.200725
iteration 367, train loss: 0.380743, validation loss: 0.231991
iteration 368, train loss: 0.3913, validation loss: 0.24703
iteration 369, train loss: 0.402753, validation loss: 0.250404
iteration 370, train loss: 0.393643, validation loss: 0.228914
iteration 371, train loss: 0.423285, validation loss: 0.202553
iteration 372, train loss: 0.366889, validation loss: 0.21978
iteration 373, train loss: 0.388742, validation loss: 0.238503
iteration 374, train loss: 0.416914, validation loss: 0.285816
iteration 375, train loss: 0.443504, validation loss: 0.243789
iteration 376, train loss: 0.408947, validation loss: 0.211881
iteration 377, train loss: 0.391657, validation loss: 0.200017
iteration 378, train loss: 0.410112, validation loss: 0.229636
iteration 379, train loss: 0.368782, validation loss: 0.289856
iteration 380, train loss: 0.431896, validation loss: 0.221891
iteration 381, train loss: 0.376957, validation loss: 0.200693
iteration 382, train loss: 0.375595, validation loss: 0.22823
iteration 383, train loss: 0.41295, validation loss: 0.194997
iteration 384, train loss: 0.393717, validation loss: 0.226237
iteration 385, train loss: 0.393423, validation loss: 0.240329
iteration 386, train loss: 0.37483, validation loss: 0.202658
iteration 387, train loss: 0.378526, validation loss: [92m0.188213[0m
iteration 388, train loss: 0.405169, validation loss: 0.204812
iteration 389, train loss: 0.411931, validation loss: 0.2312
iteration 390, train loss: 0.37019, validation loss: 0.20417
iteration 391, train loss: 0.406094, validation loss: 0.227462
iteration 392, train loss: 0.380054, validation loss: 0.211651
iteration 393, train loss: 0.402733, validation loss: 0.203165
iteration 394, train loss: 0.355207, validation loss: 0.214395
iteration 395, train loss: 0.393722, validation loss: [92m0.182047[0m
iteration 396, train loss: 0.37567, validation loss: 0.191292
iteration 397, train loss: 0.382662, validation loss: 0.200473
iteration 398, train loss: 0.399738, validation loss: 0.198173
iteration 399, train loss: 0.357391, validation loss: 0.204584
iteration 400, train loss: 0.383445, validation loss: 0.18598
iteration 401, train loss: 0.383461, validation loss: 0.184622
iteration 402, train loss: 0.353257, validation loss: 0.197255
iteration 403, train loss: 0.391322, validation loss: 0.20632
iteration 404, train loss: 0.37848, validation loss: 0.220048
iteration 405, train loss: 0.373771, validation loss: 0.187052
iteration 406, train loss: [92m0.342404[0m, validation loss: 0.18791
iteration 407, train loss: 0.381797, validation loss: 0.212924
iteration 408, train loss: 0.406391, validation loss: 0.233982
iteration 409, train loss: 0.374454, validation loss: 0.246863
iteration 410, train loss: 0.399896, validation loss: [92m0.180473[0m
iteration 411, train loss: 0.358439, validation loss: 0.206513
iteration 412, train loss: 0.394612, validation loss: 0.181128
iteration 413, train loss: 0.408391, validation loss: 0.227095
iteration 414, train loss: 0.365769, validation loss: 0.318517
iteration 415, train loss: 0.437085, validation loss: 0.228681
iteration 416, train loss: 0.385207, validation loss: 0.181929
iteration 417, train loss: 0.38641, validation loss: 0.208357
iteration 418, train loss: 0.409052, validation loss: 0.18973
iteration 419, train loss: 0.373997, validation loss: 0.218931
iteration 420, train loss: 0.376364, validation loss: 0.253423
iteration 421, train loss: 0.404952, validation loss: 0.210853
iteration 422, train loss: 0.387682, validation loss: 0.181828
iteration 423, train loss: 0.355044, validation loss: 0.204567
iteration 424, train loss: 0.389392, validation loss: 0.187879
iteration 425, train loss: 0.381079, validation loss: 0.201986
iteration 426, train loss: 0.395276, validation loss: 0.249391
iteration 427, train loss: 0.399778, validation loss: 0.246213
iteration 428, train loss: 0.392916, validation loss: 0.213836
iteration 429, train loss: 0.373979, validation loss: 0.190724
iteration 430, train loss: 0.369221, validation loss: 0.211501
iteration 431, train loss: 0.361664, validation loss: 0.214027
iteration 432, train loss: 0.394474, validation loss: 0.225616
iteration 433, train loss: 0.381151, validation loss: 0.241885
iteration 434, train loss: 0.394683, validation loss: 0.190773
iteration 435, train loss: 0.351025, validation loss: 0.181692
iteration 436, train loss: 0.377535, validation loss: 0.191806
iteration 437, train loss: 0.379948, validation loss: 0.187083
iteration 438, train loss: 0.371149, validation loss: 0.209775
iteration 439, train loss: 0.383254, validation loss: 0.214533
iteration 440, train loss: 0.379186, validation loss: 0.196081
iteration 441, train loss: 0.363708, validation loss: 0.192041
iteration 442, train loss: 0.374393, validation loss: [92m0.18008[0m
iteration 443, train loss: 0.380841, validation loss: 0.181945
iteration 444, train loss: 0.365731, validation loss: 0.197923
iteration 445, train loss: [92m0.332545[0m, validation loss: 0.202956
iteration 446, train loss: 0.377179, validation loss: 0.189828
iteration 447, train loss: 0.366379, validation loss: [92m0.169515[0m
iteration 448, train loss: 0.375209, validation loss: 0.174942
iteration 449, train loss: 0.390674, validation loss: [92m0.168843[0m
iteration 450, train loss: 0.363084, validation loss: 0.212427
iteration 451, train loss: 0.357888, validation loss: 0.230883
iteration 452, train loss: 0.377058, validation loss: 0.208728
iteration 453, train loss: 0.415017, validation loss: 0.187079
iteration 454, train loss: 0.358414, validation loss: 0.218836
iteration 455, train loss: 0.356487, validation loss: 0.187689
iteration 456, train loss: 0.381806, validation loss: 0.203391
iteration 457, train loss: 0.397505, validation loss: 0.216984
iteration 458, train loss: 0.390366, validation loss: 0.227374
iteration 459, train loss: 0.393428, validation loss: 0.204196
iteration 460, train loss: 0.37694, validation loss: 0.193364
iteration 461, train loss: 0.409411, validation loss: 0.185591
iteration 462, train loss: 0.373549, validation loss: 0.202936
iteration 463, train loss: 0.364375, validation loss: 0.234971
iteration 464, train loss: 0.403126, validation loss: 0.217997
iteration 465, train loss: 0.351887, validation loss: 0.18055
iteration 466, train loss: 0.366103, validation loss: 0.200969
iteration 467, train loss: 0.388119, validation loss: 0.185782
iteration 468, train loss: 0.398453, validation loss: 0.197085
iteration 469, train loss: 0.38458, validation loss: 0.213862
iteration 470, train loss: 0.400561, validation loss: 0.178328
iteration 471, train loss: 0.36749, validation loss: 0.177655
iteration 472, train loss: 0.352709, validation loss: 0.172806
iteration 473, train loss: 0.358909, validation loss: 0.187176
iteration 474, train loss: 0.357536, validation loss: 0.235329
iteration 475, train loss: 0.406959, validation loss: 0.191442
iteration 476, train loss: 0.355668, validation loss: 0.170663
iteration 477, train loss: 0.395826, validation loss: 0.172142
iteration 478, train loss: 0.353237, validation loss: 0.199421
iteration 479, train loss: 0.374428, validation loss: 0.180756
iteration 480, train loss: 0.349881, validation loss: 0.173264
iteration 481, train loss: 0.358655, validation loss: 0.185224
iteration 482, train loss: 0.368088, validation loss: 0.182243
iteration 483, train loss: 0.349235, validation loss: 0.203698
iteration 484, train loss: 0.375882, validation loss: 0.181901
iteration 485, train loss: 0.3605, validation loss: 0.182298
iteration 486, train loss: 0.38908, validation loss: 0.183336
iteration 487, train loss: 0.350323, validation loss: 0.200254
iteration 488, train loss: [92m0.329986[0m, validation loss: 0.201153
iteration 489, train loss: 0.351713, validation loss: 0.180216
iteration 490, train loss: 0.362383, validation loss: 0.170293
iteration 491, train loss: 0.364579, validation loss: 0.170408
iteration 492, train loss: 0.333592, validation loss: 0.170225
iteration 493, train loss: 0.366154, validation loss: 0.183919
iteration 494, train loss: 0.390204, validation loss: 0.209705
iteration 495, train loss: 0.37659, validation loss: 0.189156
iteration 496, train loss: 0.3695, validation loss: 0.186277
iteration 497, train loss: 0.362841, validation loss: 0.169012
iteration 498, train loss: 0.337278, validation loss: 0.202019
iteration 499, train loss: 0.391137, validation loss: 0.186136
iteration 500, train loss: 0.358665, validation loss: 0.186048
iteration 501, train loss: 0.361983, validation loss: 0.190643
iteration 502, train loss: [92m0.329179[0m, validation loss: 0.184967
iteration 503, train loss: 0.358897, validation loss: 0.189662
iteration 504, train loss: 0.34032, validation loss: 0.17353
iteration 505, train loss: 0.361344, validation loss: 0.186954
iteration 506, train loss: 0.356778, validation loss: 0.1987
iteration 507, train loss: 0.378236, validation loss: 0.205593
iteration 508, train loss: 0.377942, validation loss: 0.208841
iteration 509, train loss: 0.34841, validation loss: [92m0.168777[0m
iteration 510, train loss: 0.356343, validation loss: 0.18186
iteration 511, train loss: 0.373472, validation loss: 0.171982
iteration 512, train loss: 0.386418, validation loss: 0.223628
iteration 513, train loss: 0.37077, validation loss: 0.240669
iteration 514, train loss: 0.401535, validation loss: 0.173186
iteration 515, train loss: 0.363682, validation loss: 0.214728
iteration 516, train loss: 0.411752, validation loss: 0.186599
iteration 517, train loss: 0.383718, validation loss: 0.215993
iteration 518, train loss: 0.355393, validation loss: 0.288441
iteration 519, train loss: 0.410703, validation loss: 0.20001
iteration 520, train loss: 0.398001, validation loss: 0.192057
iteration 521, train loss: 0.397462, validation loss: 0.213889
iteration 522, train loss: 0.43061, validation loss: 0.196762
iteration 523, train loss: 0.34986, validation loss: 0.283866
iteration 524, train loss: 0.412312, validation loss: 0.22661
iteration 525, train loss: 0.39062, validation loss: 0.179198
iteration 526, train loss: 0.413009, validation loss: 0.195962
iteration 527, train loss: 0.40426, validation loss: 0.172816
iteration 528, train loss: 0.378987, validation loss: 0.224158
iteration 529, train loss: 0.376126, validation loss: 0.233622
iteration 530, train loss: 0.423179, validation loss: 0.171211
iteration 531, train loss: 0.343545, validation loss: 0.201297
iteration 532, train loss: 0.38095, validation loss: 0.186179
iteration 533, train loss: 0.367202, validation loss: 0.204694
iteration 534, train loss: 0.388203, validation loss: 0.245575
iteration 535, train loss: 0.373487, validation loss: 0.219034
iteration 536, train loss: 0.370872, validation loss: [92m0.16749[0m
iteration 537, train loss: 0.353591, validation loss: 0.172146
iteration 538, train loss: 0.396281, validation loss: 0.170492
iteration 539, train loss: 0.363373, validation loss: 0.202592
iteration 540, train loss: 0.360866, validation loss: 0.203736
iteration 541, train loss: 0.369472, validation loss: 0.173069
iteration 542, train loss: 0.375776, validation loss: [92m0.164405[0m
iteration 543, train loss: 0.370217, validation loss: 0.16906
iteration 544, train loss: 0.346708, validation loss: 0.190058
iteration 545, train loss: 0.349705, validation loss: 0.214887
iteration 546, train loss: 0.355549, validation loss: 0.185311
iteration 547, train loss: [92m0.326372[0m, validation loss: 0.164423
iteration 548, train loss: 0.355019, validation loss: 0.184758
iteration 549, train loss: 0.414254, validation loss: 0.171567
iteration 550, train loss: 0.361377, validation loss: 0.217379
iteration 551, train loss: 0.375225, validation loss: 0.206269
iteration 552, train loss: 0.369529, validation loss: 0.190827
iteration 553, train loss: 0.364976, validation loss: 0.179824
iteration 554, train loss: 0.37531, validation loss: 0.167057
iteration 555, train loss: 0.370557, validation loss: 0.203437
iteration 556, train loss: 0.36516, validation loss: 0.201673
iteration 557, train loss: 0.359255, validation loss: 0.178289
iteration 558, train loss: 0.327982, validation loss: 0.171032
iteration 559, train loss: 0.369634, validation loss: 0.180993
iteration 560, train loss: 0.361044, validation loss: 0.197967
iteration 561, train loss: 0.39797, validation loss: 0.186312
iteration 562, train loss: 0.349713, validation loss: 0.168972
iteration 563, train loss: 0.333176, validation loss: 0.176086
iteration 564, train loss: 0.366749, validation loss: 0.166042
iteration 565, train loss: 0.339317, validation loss: 0.179652
iteration 566, train loss: 0.366372, validation loss: 0.197153
iteration 567, train loss: 0.369278, validation loss: 0.177855
iteration 568, train loss: 0.328977, validation loss: [92m0.162159[0m
iteration 569, train loss: 0.353764, validation loss: [92m0.16209[0m
iteration 570, train loss: 0.366503, validation loss: 0.190202
iteration 571, train loss: 0.339081, validation loss: 0.195442
iteration 572, train loss: 0.342487, validation loss: 0.203844
iteration 573, train loss: 0.375592, validation loss: 0.197874
iteration 574, train loss: 0.362117, validation loss: 0.163386
iteration 575, train loss: 0.356684, validation loss: 0.205758
iteration 576, train loss: 0.405667, validation loss: 0.180398
iteration 577, train loss: 0.358128, validation loss: 0.181203
iteration 578, train loss: 0.35144, validation loss: 0.20804
iteration 579, train loss: 0.360978, validation loss: 0.193783
iteration 580, train loss: 0.35731, validation loss: 0.17163
iteration 581, train loss: 0.347367, validation loss: 0.191534
iteration 582, train loss: 0.383998, validation loss: 0.178911
iteration 583, train loss: 0.357721, validation loss: 0.166286
iteration 584, train loss: 0.364994, validation loss: 0.170235
iteration 585, train loss: 0.341047, validation loss: 0.170423
iteration 586, train loss: 0.363678, validation loss: 0.19481
iteration 587, train loss: 0.36326, validation loss: 0.191784
iteration 588, train loss: 0.332898, validation loss: 0.172519
iteration 589, train loss: 0.333416, validation loss: 0.165721
iteration 590, train loss: 0.34691, validation loss: 0.171356
iteration 591, train loss: 0.363728, validation loss: 0.174634
iteration 592, train loss: 0.351295, validation loss: 0.192417
iteration 593, train loss: 0.340098, validation loss: 0.191185
iteration 594, train loss: 0.353675, validation loss: 0.162655
iteration 595, train loss: 0.33009, validation loss: 0.166475
iteration 596, train loss: 0.330826, validation loss: 0.167853
iteration 597, train loss: 0.345006, validation loss: 0.180793
iteration 598, train loss: 0.350983, validation loss: 0.182239
iteration 599, train loss: 0.343986, validation loss: 0.177131
MSE: 0.45144365
Total R2: -1.0026370374457687
Projected R2: -1.1123057938704846
Plotting random trials
Analyzing fixed points
Calculating Line Attractor analytics
Analyzing points on a line attractor in motion context...
Analyzing points on a line attractor in color context...
0.0082221_CDDM;relu;N=96;lmbdo=0.3;lmbdr=0.3;lr=0.002;maxiter=3000 2
seed: 353750
Using cuda for RNN!
Using cuda for Latent Circuit!
setting projection of RNN traces on the lower subspace
iteration 0, train loss: [92m3.45657[0m, validation loss: [92m2.917351[0m
iteration 1, train loss: [92m2.995976[0m, validation loss: [92m2.595123[0m
iteration 2, train loss: [92m2.671115[0m, validation loss: [92m2.374122[0m
iteration 3, train loss: [92m2.45127[0m, validation loss: [92m2.224941[0m
iteration 4, train loss: [92m2.308667[0m, validation loss: [92m2.120394[0m
iteration 5, train loss: [92m2.208238[0m, validation loss: [92m2.03999[0m
iteration 6, train loss: [92m2.126008[0m, validation loss: [92m1.973902[0m
iteration 7, train loss: [92m2.057631[0m, validation loss: [92m1.918354[0m
iteration 8, train loss: [92m2.008861[0m, validation loss: [92m1.871513[0m
iteration 9, train loss: [92m1.956511[0m, validation loss: [92m1.83163[0m
iteration 10, train loss: [92m1.914898[0m, validation loss: [92m1.797045[0m
iteration 11, train loss: [92m1.882417[0m, validation loss: [92m1.766181[0m
iteration 12, train loss: [92m1.844142[0m, validation loss: [92m1.736421[0m
iteration 13, train loss: [92m1.815279[0m, validation loss: [92m1.706679[0m
iteration 14, train loss: [92m1.786785[0m, validation loss: [92m1.677326[0m
iteration 15, train loss: [92m1.758212[0m, validation loss: [92m1.649857[0m
iteration 16, train loss: [92m1.730712[0m, validation loss: [92m1.624997[0m
iteration 17, train loss: [92m1.707171[0m, validation loss: [92m1.601863[0m
iteration 18, train loss: [92m1.681672[0m, validation loss: [92m1.580795[0m
iteration 19, train loss: [92m1.66207[0m, validation loss: [92m1.561542[0m
iteration 20, train loss: [92m1.650245[0m, validation loss: [92m1.542946[0m
iteration 21, train loss: [92m1.637983[0m, validation loss: [92m1.523436[0m
iteration 22, train loss: [92m1.613846[0m, validation loss: [92m1.502578[0m
iteration 23, train loss: [92m1.594161[0m, validation loss: [92m1.481414[0m
iteration 24, train loss: [92m1.572071[0m, validation loss: [92m1.461531[0m
iteration 25, train loss: [92m1.550238[0m, validation loss: [92m1.443746[0m
iteration 26, train loss: [92m1.539097[0m, validation loss: [92m1.427362[0m
iteration 27, train loss: [92m1.510897[0m, validation loss: [92m1.410866[0m
iteration 28, train loss: [92m1.500758[0m, validation loss: [92m1.393359[0m
iteration 29, train loss: [92m1.481987[0m, validation loss: [92m1.375895[0m
iteration 30, train loss: [92m1.469022[0m, validation loss: [92m1.360015[0m
iteration 31, train loss: [92m1.450917[0m, validation loss: [92m1.346214[0m
iteration 32, train loss: [92m1.440681[0m, validation loss: [92m1.33387[0m
iteration 33, train loss: [92m1.422649[0m, validation loss: [92m1.322861[0m
iteration 34, train loss: [92m1.412638[0m, validation loss: [92m1.314144[0m
iteration 35, train loss: [92m1.401863[0m, validation loss: [92m1.306447[0m
iteration 36, train loss: [92m1.3